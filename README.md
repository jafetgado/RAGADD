# RAGADD: Retrieval augmented generation (RAG) for question answering about ADD/ADHD medication
-------------

This project applies Retrieval-Augmented Generation (RAG) for answering questions about ADHD medications based on patient-written drug reviews. 
The goal is to build a RAG-based system that performs competitively with a strong ground-truth baseline generated by GPT-4o with full access to all patient reviews (n=4,509).

## Overview

We use the [`drugscom_reviews`](https://huggingface.co/datasets/Zakia/drugscom_reviews) dataset to extract and preprocess reviews related to ADHD medications. These reviews are used to build a retrieval-augmented QA system. 
The RAG system is evaluated on its ability to generate accurate answers to 50 user-centered questions derived from the dataset, compared with zero-shot baselines.

## Approach

### 1. Prepare Dataset

- Extracted ADHD-related reviews from the `Zakia/drugscom_reviews` dataset on HuggingFace.
- Preprocessed data to remove long reviews (>1000 characters) and HTML characters. 
- Converted reviews into `LangChain` `Document` objects and stored them for use in vector search.
- Generated 50 evaluation questions using GPT-4o, ensuring they were grounded in the content of the reviews.
- Used GPT-4o again to generate ground-truth answers for evaluation with all review documents provided in the context (n=4,509 documents).

### 2. Optimize Hyperparameters

- Derived a subset of 10 representative questions from the full test set for validation. Used the validation set to optimize hyperparameters, and the full test set for final performance evaluation.
- Compared multiple embedding models:
  - `text-embedding-3-small`
  - `text-embedding-3-large` (optimal)
  - `BioSimCSE-BioLinkBERT`
  - `BMRetriever`
- Tested two FAISS similarity metrics:
  - L2 (Euclidean) distance
  - Dot product (inner product) (optimial)
- Grid-searched over RAG/generation parameters:
  - `k`: number of retrieved documents (1â€“20) (optimal = 15)
  - `temperature`: [0.1, 0.25, 0.5, 0.75, 1.0, 1.25] (optimal =0.25)
  - `top_p`: [0.25, 0.5, 0.75, 1.0] (optimal = 1.0)

### 3. Generate RAG Responses

- Implemented generation with OpenAI API with LangChain pipelines.
- Compared three generation settings on test data:
  - Zero-shot GPT-3.5-turbo (baseline)
  - Zero-shot GPT-4o (baseline)
  - GPT-3.5-turbo with RAG using optimized settings
- Evaluated output using ROUGE and BERTScore (F1), selection based on BERTScore. Used BERTScore for selection to focus on semantic similarity rather than exact word or n-gram matching.

## Results

| Model                  | BERTScore (F1) |
|------------------------|----------------|
| GPT-4o (zero-shot)     |  0.488         |
| GPT-3.5 (zero-shot)    |  0.457         |
| GPT-3.5 (RAG, optimal) |  **0.501**     |


## Key Takeaways

- After careful optimization of hyperparameters, retrieval-augmented generation significantly improves answer quality, outperforming the zeroshot baselines.
- The well-optimized GPT-3.5 RAG system is competitive with GPT-4o with full document access.

## Files

- `01_prepare_dataset.ipynb`: Data extraction, cleaning, question/answer generation.
- `02_optimize_hyperparameters.ipynb`: Embedding and RAG parameter optimization.
- `03_generate_RAG_responses.ipynb`: Full test evaluation and performance comparison.

## Example Responses

**Question**:
```
How do ADHD medications affect gut health?
```